# Requests库
## 方法

| 方法               | 说明                                          |
| ------------------ | --------------------------------------------- |
| requests.request() | 构造一个请求，支撑以下各方法的基础方法        |
| requests.get()     | 对应于HTTP的GET                               |
| requests.head()    | 对应于HTTP的HEAD，获取HTML网页头信息的方法    |
| requests.post()    | 对应于HTTP的POST                              |
| requests.put()     | 对应于HTTP的PUT                               |
| requests.patch()   | 对应于HTTP的PATCH，向HTML网页提交局部修改请求 |
| requests.delete()  | 对应于HTTP的DELETE                            |

格式为

```python
requests.get(url, params=None, **kwargs)
requests.head(url, **kwargs)
requests.post(url, data=None, json=None, **kwargs)
requests.put(url, data=None, **kwargs)
requests.patch(url, data=None, **kwargs)
requests.delete(url, **kwargs)
```
其中
- params: 字典或字节序列，作为参数增加到url中
- **kwargs: 12个控制访问的参数
    - data: 字典、字节序列或文件对象，作为Request的内容
    - json: JSON格式的数据，作为Request的内容
    - headers: 字典，HTTP定制头 
        - User-Agent: 可以用于伪装成浏览器
            ```python
            kv = {'user-agent': 'Mozilla/5.0'}
            r = requests.get(url, headers=kv)
            ```

    - cookies: 字典或CookieJar，Request中的cookie
    - auth: 元组，支持HTTP认证功能
    - files: 字典类型，传输文件
    - timeout: 设定超时时间，秒为单位
    - proxies: 字典类型，设定访问代理服务器，可以增加登录认证
    - allow_redirects: True/False，默认为True，重定向开关
    - stream: True/False，默认为True，获取内容立即下载开关
    - verify: True/False，默认为True，认证SSL证书开关
    - cert: 本地SSL证书路径

## Response对象

Response 对象具有以下属性

- `r.status_code`: HTTP请求的返回状态，200表示连接成功，404表示失败
- `r.text`
- `r.encoding`: 从HTTP header中猜测的响应内容编码方式
- `r.apparent_encoding`: 从内容中分析出的响应内容编码方式（备选编码方式）
- `r.content`: 这是二进制形式，不同于`r.text`

![image-20231113165318827](https://img2023.cnblogs.com/blog/1892247/202311/1892247-20231113165325122-2073012228.png)

## 异常
|异常|说明|
|---|---|
|requests.ConnectionError|网络连接错误异常，如DNS查询失败、拒绝连接等|
|requests.HTTPError|HTTP错误异常|
|requests.URLRequired|URL缺失异常|
|requests.TooManyRedirects|超过最大重定向次数，产生重定向异常|
|requests.ConnectTimeout|连接远程服务器超时异常|
|requests.Timeout|请求URL超时，产生超时异常|

一个处理异常的框架代码
```python
def getHTMLText(url):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status() # 如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "产生异常"
```

## Robots协议

参考一个京东的 [robots协议](https://www.jd.com/robots.txt)

```txt
User‐agent: *
Disallow: /?*
Disallow: /pop/*.html
Disallow: /pinpai/*.html?*
User‐agent: EtaoSpider
Disallow: /
User‐agent: HuihuiSpider
Disallow: /
User‐agent: GwdangSpider
Disallow: /
User‐agent: WochachaSpider
Disallow: /
```

# Beautiful Soup库
## 基本元素
看以下的例子，基本上就知道基本元素怎么用了
```python
>>> demo = requests.get("https://python123.io/ws/demo.html").text
>>> demo
'<html><head><title>This is a python demo page</title></head>\r\n<body>\r\n<p class="title"><b>The demo python introduces several python courses.</b></p>\r\n<p class="course">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n<a href="http://www.icourse163.org/course/BIT-268001" class="py1" id="link1">Basic Python</a> and <a href="http://www.icourse163.org/course/BIT-1001870001" class="py2" id="link2">Advanced Python</a>.</p>\r\n</body></html>'
>>> from bs4 import BeautifulSoupfrom
>>> soup = BeautifulSoup(demo, "html.parser")
>>> soup.title
<title>This is a python demo page</title>
>>> tag = soup.a # 如果有多个则对应第一个
>>> tag 
<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>
>>> tag.name
'a'
>>> tag.attrs
{'href': 'http://www.icourse163.org/course/BIT-268001', 'class': ['py1'], 'id': 'link1'}
>>> tag.string
'Basic Python'
>>> tag.parent.name
'p'

>>> newsoup = BeautifulSoup("<b><!--This is a comment--></b><p>This is not a comment</p>", "html.parser")
>>> newsoup.b.string
'This is a comment'
>>> type(newsoup.b.string)
<class 'bs4.element.Comment'>
>>> newsoup.p.string
'This is not a comment'
>>> type(newsoup.p.string)
<class 'bs4.element.NavigableString'>
```
## 遍历方式
- 上行: `.parent`, `.parents`
- 下行: `.contents`, `.children`, `.descendants`
- 平行: `.next_sibling`, `.previous_sibling`, `.next_siblings`, `.previous_siblings`
其中 `.contents` 返回一个列表, 其他返回一个元素或者迭代器

## 信息提取

`<tag>(..)` 等价于 `<tag>.find_all(..)`
`soup(..)` 等价于 `soup.find_all(..)`
标准格式为
```python
<>.find_all(name, attrs, recursive, string, **kwargs)
```
返回一个*列表类型*，存储查找的结果，其中
- **name**: 对标签名称的检索字符串
- **attrs**: 对标签属性值的检索字符串，可标注属性检索
- **recursive**: 是否对子孙全部检索，默认为True
- **string**: <>...</>中字符串区域的检索字符串

# 实战之大学排名爬取

注意一个数据输出时的格式化格式

![image-20231113210534390](https://img2023.cnblogs.com/blog/1892247/202311/1892247-20231113210553183-425344001.png)

代码如下
```python
import requests
import bs4
from bs4 import BeautifulSoup


def getHTMLText(url):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()  # 如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "Exception"


def fillUnivList(ulist, html):
    soup = BeautifulSoup(html, "html.parser")
    for tr in soup.find('tbody').children:
        if isinstance(tr, bs4.element.Tag):  # 判断tr是否为bs4库中的Tag类型
            name_cn = tr.find_all(attrs={'class': 'name-cn'}, recursive=True)[0].get_text(strip=True)
            td = tr.find_all('td')
            UnivRank = td[0].get_text(strip=True)
            UnivLocation = td[2].get_text(strip=True)
            UnivType = td[3].get_text(strip=True)
            # 去除空格
            ulist.append([UnivRank, name_cn, UnivLocation, UnivType])


def printUnivList(ulist, num):
    print("{:^10}\t{:^20}\t{:^10}\t{:^10}".format("排名", "学校名称", "所在地", "类型"))
    for i in range(num):
        u = ulist[i]
        print("{:^10}\t{:^20}\t{:^10}\t{:^10}".format(u[0], u[1], u[2], u[3]))


def main():
    uinfo = []
    url = "https://www.shanghairanking.cn/rankings/bcur/2023"
    html = getHTMLText(url)
    fillUnivList(uinfo, html)
    printUnivList(uinfo, 20)  # 20 univs


if __name__ == '__main__':
    main()
```